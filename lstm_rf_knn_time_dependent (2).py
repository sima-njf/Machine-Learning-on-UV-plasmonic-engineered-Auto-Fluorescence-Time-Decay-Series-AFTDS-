# -*- coding: utf-8 -*-
"""LSTM-RF-KNN/time-dependent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pWAeZmwUI7YvyjIdu7FakxSk1_LzTpPl
"""

# AFTDS Time-Series Classification Pipeline
# Supports KNN, Random Forest, and LSTM models

import os
import zipfile
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import pickle
from tensorflow.keras.models import load_model

from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization, Masking
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping

# Step 1: Load and parse waveform data from zip archive
def extract_and_parse(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as z:
        z.extractall(extract_to)
    data = []
    for root, _, files in os.walk(extract_to):
        for file in files:
            if file.endswith('.txt') and 'Baseline' not in file:
                full_path = os.path.join(root, file)
                with open(full_path, 'r') as f:
                    lines = [list(map(float, l.split())) for l in f if l.strip()]
                label = file.split('_')[-4]
                data.append((np.array(lines), label))
    return data

# Step 2: Flatten data for KNN/RF

def prepare_classical(data):
    X, y = [], []
    for seq, label in data:
        X.append(seq.flatten())
        y.append(label)
    return np.array(X), np.array(y)

# Step 3: Prepare fixed-length padded sequences for LSTM

def prepare_lstm(data, max_len=100):
    X, y = [], []
    for seq, label in data:
        padded = np.zeros((max_len, 2))
        padded[:min(len(seq), max_len)] = seq[:max_len]
        X.append(padded)
        y.append(label)
    return np.array(X), np.array(y)

# Step 4: Evaluate classical models

def run_classical_models(X, y):
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.25, stratify=y_encoded)

    models = {
        "KNN": KNeighborsClassifier(n_neighbors=5),
        "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42)
    }

    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        print(f"\n{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}")
        print(classification_report(y_test, y_pred, target_names=le.classes_))
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=le.classes_, yticklabels=le.classes_)
        plt.title(f"{name} Confusion Matrix")
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.show()

# Step 5: Build and run LSTM

def run_lstm_model(X, y):
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    y_cat = to_categorical(y_encoded)
    X = MinMaxScaler().fit_transform(X.reshape((X.shape[0], -1))).reshape(X.shape)

    X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.25, stratify=y_encoded)

    model = Sequential([
        LSTM(512, activation='relu', return_sequences=True, input_shape=X.shape[1:]),
        BatchNormalization(), Dropout(0.2),
        LSTM(256, activation='relu'),
        BatchNormalization(), Dropout(0.2),
        Dense(100, activation='relu'),
        BatchNormalization(), Dropout(0.2),
        Dense(y_cat.shape[1], activation='softmax')
    ])

    model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])
    lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)
    early_stop = EarlyStopping(patience=5, restore_best_weights=True)

    history = model.fit(
        X_train, y_train, validation_split=0.2,
        epochs=50, batch_size=64, callbacks=[lr_reduce, early_stop], verbose=1
    )

    # Load previous training history if needed
    try:
        with open('/content/drive/MyDrive/chemistry_model/bestmodel_history.pkl', 'rb') as file:
            model_history = pickle.load(file)
            print("✅ Loaded previous training history.")
    except Exception as e:
        print(f"⚠️ Could not load training history: {e}")

    loss, acc = model.evaluate(X_test, y_test)
    print(f"\nLSTM Accuracy: {acc:.4f}")
    y_pred = model.predict(X_test).argmax(axis=1)
    y_true = y_test.argmax(axis=1)
    print(classification_report(y_true, y_pred, target_names=le.classes_))
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Purples", xticklabels=le.classes_, yticklabels=le.classes_)
    plt.title("LSTM Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# === Entry Point ===
if __name__ == '__main__':
    raw_data = extract_and_parse('Data_ML.zip', 'unzipped')
    X_clf, y_clf = prepare_classical(raw_data)
    run_classical_models(X_clf, y_clf)

    X_lstm, y_lstm = prepare_lstm(raw_data, max_len=100)
    run_lstm_model(X_lstm, y_lstm)
